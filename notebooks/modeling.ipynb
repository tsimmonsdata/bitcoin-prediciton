{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an LSTM model to predict future bitcoin prices given the return values and trend values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv, DataFrame, concat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Activation, Dense, Dropout, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and separate dates\n",
    "btc_df = read_csv('../data/processed/price_trend_data.csv')\n",
    "dates = btc_df.date\n",
    "dataset_raw = btc_df.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data so that y is the next time step with forecast sequence\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "# reshape time series\n",
    "look_back = 10\n",
    "dataset = series_to_supervised(dataset_raw, n_in=look_back)\n",
    "# drop var2(t) since var1(t) is target\n",
    "dataset.drop(dataset.columns[len(dataset.columns)-1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-10)</th>\n",
       "      <th>var2(t-10)</th>\n",
       "      <th>var1(t-9)</th>\n",
       "      <th>var2(t-9)</th>\n",
       "      <th>var1(t-8)</th>\n",
       "      <th>var2(t-8)</th>\n",
       "      <th>var1(t-7)</th>\n",
       "      <th>var2(t-7)</th>\n",
       "      <th>var1(t-6)</th>\n",
       "      <th>var2(t-6)</th>\n",
       "      <th>...</th>\n",
       "      <th>var2(t-5)</th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>var2(t-4)</th>\n",
       "      <th>var1(t-3)</th>\n",
       "      <th>var2(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var2(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.122602</td>\n",
       "      <td>0.039388</td>\n",
       "      <td>0.109199</td>\n",
       "      <td>0.020147</td>\n",
       "      <td>-0.148420</td>\n",
       "      <td>0.048620</td>\n",
       "      <td>0.076961</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>0.259511</td>\n",
       "      <td>0.020946</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025209</td>\n",
       "      <td>-0.101783</td>\n",
       "      <td>0.051031</td>\n",
       "      <td>-0.036368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.109199</td>\n",
       "      <td>0.020147</td>\n",
       "      <td>-0.148420</td>\n",
       "      <td>0.048620</td>\n",
       "      <td>0.076961</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>0.259511</td>\n",
       "      <td>0.020946</td>\n",
       "      <td>-0.121361</td>\n",
       "      <td>-0.172811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025209</td>\n",
       "      <td>-0.101783</td>\n",
       "      <td>0.051031</td>\n",
       "      <td>-0.036368</td>\n",
       "      <td>-0.057106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>-0.148420</td>\n",
       "      <td>0.048620</td>\n",
       "      <td>0.076961</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>0.259511</td>\n",
       "      <td>0.020946</td>\n",
       "      <td>-0.121361</td>\n",
       "      <td>-0.172811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025209</td>\n",
       "      <td>-0.101783</td>\n",
       "      <td>0.051031</td>\n",
       "      <td>-0.036368</td>\n",
       "      <td>-0.057106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.025840</td>\n",
       "      <td>0.036368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.076961</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>0.259511</td>\n",
       "      <td>0.020946</td>\n",
       "      <td>-0.121361</td>\n",
       "      <td>-0.172811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025209</td>\n",
       "      <td>-0.101783</td>\n",
       "      <td>0.051031</td>\n",
       "      <td>-0.036368</td>\n",
       "      <td>-0.057106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.025840</td>\n",
       "      <td>0.036368</td>\n",
       "      <td>-0.009472</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.259511</td>\n",
       "      <td>0.020946</td>\n",
       "      <td>-0.121361</td>\n",
       "      <td>-0.172811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051031</td>\n",
       "      <td>-0.036368</td>\n",
       "      <td>-0.057106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.025840</td>\n",
       "      <td>0.036368</td>\n",
       "      <td>-0.009472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032621</td>\n",
       "      <td>0.164303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    var1(t-10)  var2(t-10)  var1(t-9)  var2(t-9)  var1(t-8)  var2(t-8)  \\\n",
       "10    0.122602    0.039388   0.109199   0.020147  -0.148420   0.048620   \n",
       "11    0.109199    0.020147  -0.148420   0.048620   0.076961   0.082611   \n",
       "12   -0.148420    0.048620   0.076961   0.082611   0.259511   0.020946   \n",
       "13    0.076961    0.082611   0.259511   0.020946  -0.121361  -0.172811   \n",
       "14    0.259511    0.020946  -0.121361  -0.172811   0.000000   0.049526   \n",
       "\n",
       "    var1(t-7)  var2(t-7)  var1(t-6)  var2(t-6)  ...  var2(t-5)  var1(t-4)  \\\n",
       "10   0.076961   0.082611   0.259511   0.020946  ...  -0.172811   0.000000   \n",
       "11   0.259511   0.020946  -0.121361  -0.172811  ...   0.049526   0.000000   \n",
       "12  -0.121361  -0.172811   0.000000   0.049526  ...   0.004901   0.000000   \n",
       "13   0.000000   0.049526   0.000000   0.004901  ...   0.025209  -0.101783   \n",
       "14   0.000000   0.004901   0.000000   0.025209  ...   0.051031  -0.036368   \n",
       "\n",
       "    var2(t-4)  var1(t-3)  var2(t-3)  var1(t-2)  var2(t-2)  var1(t-1)  \\\n",
       "10   0.049526   0.000000   0.004901   0.000000   0.025209  -0.101783   \n",
       "11   0.004901   0.000000   0.025209  -0.101783   0.051031  -0.036368   \n",
       "12   0.025209  -0.101783   0.051031  -0.036368  -0.057106   0.000000   \n",
       "13   0.051031  -0.036368  -0.057106   0.000000  -0.025840   0.036368   \n",
       "14  -0.057106   0.000000  -0.025840   0.036368  -0.009472   0.000000   \n",
       "\n",
       "    var2(t-1)   var1(t)  \n",
       "10   0.051031 -0.036368  \n",
       "11  -0.057106  0.000000  \n",
       "12  -0.025840  0.036368  \n",
       "13  -0.009472  0.000000  \n",
       "14   0.032621  0.164303  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset.values[:train_size], dataset.values[train_size:len(dataset)]\n",
    "\n",
    "# split in X and y\n",
    "X_train, y_train = train[:, :-1], train[:, -1]\n",
    "X_test, y_test = test[:, :-1], test[:, -1]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features] for LSTM input\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# scale on training data\n",
    "scalers = {}\n",
    "for i in range(X_train.shape[2]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i])\n",
    "\n",
    "for i in range(X_test.shape[2]):\n",
    "    X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "    \n",
    "scale_y = StandardScaler()\n",
    "y_train = scale_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = scale_y.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy comparison models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training dummy model on training data with strategy mean ...\n",
      "\n",
      "Performance on train data:\n",
      "mean MSE: 0.9999999999999999\n",
      "\n",
      "Performance on test data:\n",
      "mean MSE: 1.0349459376817913\n",
      "\n",
      "Training dummy model on training data with strategy median ...\n",
      "\n",
      "Performance on train data:\n",
      "median MSE: 1.0000765511111582\n",
      "\n",
      "Performance on test data:\n",
      "median MSE: 1.0345844877355812\n",
      "\n",
      "Training dummy model on training data with strategy constant ...\n",
      "\n",
      "Performance on train data:\n",
      "constant MSE: 0.9999999999999999\n",
      "\n",
      "Performance on test data:\n",
      "constant MSE: 1.0349459376817913\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate Dummy Models.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "for strategy in ['mean', 'median', 'constant']:\n",
    "    \n",
    "    if strategy=='constant':\n",
    "        dummy=DummyRegressor(strategy=strategy, constant=(0,))\n",
    "    else:\n",
    "        dummy=DummyRegressor(strategy=strategy)\n",
    "        \n",
    "    print('\\nTraining dummy model on training data with strategy {} ...'.format(strategy))\n",
    "    dummy.fit(X_train, y_train)\n",
    "    \n",
    "    print('\\nPerformance on train data:')\n",
    "    dummy_predict_train = dummy.predict(X_train)\n",
    "    print('{} MSE: {}'.format(strategy, mean_squared_error(y_true=y_train, y_pred=dummy_predict_train)))\n",
    "    \n",
    "    print('\\nPerformance on test data:')\n",
    "    dummy_predict_test = dummy.predict(X_test)\n",
    "    print('{} MSE: {}'.format(strategy, mean_squared_error(y_true=y_test, y_pred=dummy_predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is just an expository project we aren't looking for fantastic results. For one, the dataset being used is very small compared to the data required to get good results with an LSTM. Additionally, the parameters dropout, nodes, epochs, and batch_size were tuned by hand due to computation constraints. With more resources the parameters could be better tuned by using a nested cross-validation technique such as day forward-chaining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\envs\\capstone\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\envs\\capstone\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\envs\\capstone\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\envs\\capstone\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\envs\\capstone\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\envs\\capstone\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\envs\\capstone\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 1465 samples, validate on 629 samples\n",
      "Epoch 1/100\n",
      "1465/1465 [==============================] - 4s 2ms/step - loss: 0.9936 - val_loss: 1.0100\n",
      "Epoch 2/100\n",
      "1465/1465 [==============================] - 1s 477us/step - loss: 0.9735 - val_loss: 0.9812\n",
      "Epoch 3/100\n",
      "1465/1465 [==============================] - 1s 487us/step - loss: 0.9541 - val_loss: 0.9403\n",
      "Epoch 4/100\n",
      "1465/1465 [==============================] - 1s 390us/step - loss: 0.9206 - val_loss: 0.8940\n",
      "Epoch 5/100\n",
      "1465/1465 [==============================] - 1s 460us/step - loss: 0.9214 - val_loss: 0.8720\n",
      "Epoch 6/100\n",
      "1465/1465 [==============================] - 1s 426us/step - loss: 0.9090 - val_loss: 0.8636\n",
      "Epoch 7/100\n",
      "1465/1465 [==============================] - 1s 405us/step - loss: 0.8853 - val_loss: 0.8448\n",
      "Epoch 8/100\n",
      "1465/1465 [==============================] - 1s 408us/step - loss: 0.8858 - val_loss: 0.8419\n",
      "Epoch 9/100\n",
      "1465/1465 [==============================] - 1s 386us/step - loss: 0.8896 - val_loss: 0.8353\n",
      "Epoch 10/100\n",
      "1465/1465 [==============================] - 1s 441us/step - loss: 0.8488 - val_loss: 0.8327\n",
      "Epoch 11/100\n",
      "1465/1465 [==============================] - 1s 443us/step - loss: 0.8512 - val_loss: 0.8274\n",
      "Epoch 12/100\n",
      "1465/1465 [==============================] - 1s 427us/step - loss: 0.8502 - val_loss: 0.8302\n",
      "Epoch 13/100\n",
      "1465/1465 [==============================] - 1s 503us/step - loss: 0.8188 - val_loss: 0.8255\n",
      "Epoch 14/100\n",
      "1465/1465 [==============================] - 1s 359us/step - loss: 0.8258 - val_loss: 0.8255\n",
      "Epoch 15/100\n",
      "1465/1465 [==============================] - 1s 346us/step - loss: 0.8031 - val_loss: 0.8283\n",
      "Epoch 16/100\n",
      "1465/1465 [==============================] - 1s 344us/step - loss: 0.8258 - val_loss: 0.8221\n",
      "Epoch 17/100\n",
      "1465/1465 [==============================] - 1s 342us/step - loss: 0.8032 - val_loss: 0.8223\n",
      "Epoch 18/100\n",
      "1465/1465 [==============================] - 0s 340us/step - loss: 0.8205 - val_loss: 0.8224\n",
      "Epoch 19/100\n",
      "1465/1465 [==============================] - 0s 331us/step - loss: 0.8232 - val_loss: 0.8226\n",
      "Epoch 20/100\n",
      "1465/1465 [==============================] - 1s 344us/step - loss: 0.7966 - val_loss: 0.8217\n",
      "Epoch 21/100\n",
      "1465/1465 [==============================] - 0s 333us/step - loss: 0.7993 - val_loss: 0.8155\n",
      "Epoch 22/100\n",
      "1465/1465 [==============================] - 1s 363us/step - loss: 0.8029 - val_loss: 0.8139\n",
      "Epoch 23/100\n",
      "1465/1465 [==============================] - 1s 376us/step - loss: 0.8045 - val_loss: 0.8141\n",
      "Epoch 24/100\n",
      "1465/1465 [==============================] - 1s 382us/step - loss: 0.7899 - val_loss: 0.8092\n",
      "Epoch 25/100\n",
      "1465/1465 [==============================] - 0s 320us/step - loss: 0.7773 - val_loss: 0.8117\n",
      "Epoch 26/100\n",
      "1465/1465 [==============================] - 1s 347us/step - loss: 0.8147 - val_loss: 0.8031\n",
      "Epoch 27/100\n",
      "1465/1465 [==============================] - 1s 400us/step - loss: 0.8026 - val_loss: 0.8042\n",
      "Epoch 28/100\n",
      "1465/1465 [==============================] - 1s 352us/step - loss: 0.7951 - val_loss: 0.8017\n",
      "Epoch 29/100\n",
      "1465/1465 [==============================] - 1s 349us/step - loss: 0.7837 - val_loss: 0.7925\n",
      "Epoch 30/100\n",
      "1465/1465 [==============================] - 1s 430us/step - loss: 0.7842 - val_loss: 0.7956\n",
      "Epoch 31/100\n",
      "1465/1465 [==============================] - 1s 369us/step - loss: 0.7515 - val_loss: 0.7940\n",
      "Epoch 32/100\n",
      "1465/1465 [==============================] - 1s 366us/step - loss: 0.7816 - val_loss: 0.7945\n",
      "Epoch 33/100\n",
      "1465/1465 [==============================] - 0s 335us/step - loss: 0.7543 - val_loss: 0.7904\n",
      "Epoch 34/100\n",
      "1465/1465 [==============================] - 1s 385us/step - loss: 0.7792 - val_loss: 0.7927\n",
      "Epoch 35/100\n",
      "1465/1465 [==============================] - 1s 361us/step - loss: 0.7543 - val_loss: 0.8022\n",
      "Epoch 36/100\n",
      "1465/1465 [==============================] - 0s 326us/step - loss: 0.7830 - val_loss: 0.8001\n",
      "Epoch 37/100\n",
      "1050/1465 [====================>.........] - ETA: 0s - loss: 0.7265"
     ]
    }
   ],
   "source": [
    "dropout = 0.8\n",
    "nodes = 64\n",
    "epochs = 100\n",
    "batch_size = 25\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(nodes, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(LSTM(nodes))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=epochs, verbose=1, shuffle=False, \n",
    "                    batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "# make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train and validation loss per epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and validation loss don't quite converge but with such small data these were the best results with tuning by hand. Tuning either way gives clear over- and under-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate mean squared error\n",
    "train_score = mean_squared_error(y_train, train_predict[:,0])\n",
    "print('Train Score: %.4f MSE' % (train_score))\n",
    "test_score = mean_squared_error(y_test, test_predict[:,0])\n",
    "print('Test Score: %.4f MSE' % (test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly better than the dummy cases. Would like to compare to other models such as ARIMA or XGBOOST as well as compare results without using the trend variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inverse scale target and predictions\n",
    "y_train = scale_y.inverse_transform(y_train)\n",
    "y_test = scale_y.inverse_transform(y_test)\n",
    "test_predict = scale_y.inverse_transform(test_predict)\n",
    "train_predict = scale_y.inverse_transform(train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare predicitons on test data\n",
    "plt.plot(y_test)\n",
    "plt.plot(test_predict)\n",
    "plt.xlabel('Time Step')\n",
    "plt.title('Predicted vs. Actual on Test Data')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# closer look\n",
    "plt.plot(y_test[50:100])\n",
    "plt.plot(test_predict[50:100])\n",
    "plt.title('Test Predictions')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential problem with using LSTMs on time-series data is the possibility of the model learning a lagged version of the input, we can see that this isn't the case with these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare predictions on train data\n",
    "plt.plot(y_train)\n",
    "plt.plot(train_predict)\n",
    "plt.xlabel('Time Step')\n",
    "plt.title('Predicted vs. Actual on Train Data')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_train[50:100])\n",
    "plt.plot(train_predict[50:100])\n",
    "plt.title('Train Predictions')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the small amount of data used to train the model and the simplicity of the model these results seem like a promising starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
